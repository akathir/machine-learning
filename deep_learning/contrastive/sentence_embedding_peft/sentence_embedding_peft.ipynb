{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47ccdfb-72d2-4e3d-9348-156e6c8c3a01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    html {\n",
       "        font-size: 18px !important;\n",
       "    }\n",
       "\n",
       "    body {\n",
       "        background-color: #FFF !important;\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    body .notebook-app {\n",
       "        background-color: #FFF !important;\n",
       "    }\n",
       "\n",
       "    #header {\n",
       "        box-shadow: none !important;\n",
       "    }\n",
       "\n",
       "    #notebook {\n",
       "        padding-top: 0px;\n",
       "    }\n",
       "\n",
       "    #notebook-container {\n",
       "        box-shadow: none;\n",
       "        -webkit-box-shadow: none;\n",
       "        padding: 10px;\n",
       "    }\n",
       "\n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border: 1px dashed #CCCCCC;\n",
       "    }\n",
       "\n",
       "    .edit_mode div.cell.selected {\n",
       "        border: 1px dashed #828282;\n",
       "    }\n",
       "\n",
       "    div.output_wrapper {\n",
       "        margin-top: 8px;\n",
       "    }\n",
       "\n",
       "    a {\n",
       "        color: #383838;\n",
       "    }\n",
       "\n",
       "    code,\n",
       "    kbd,\n",
       "    pre,\n",
       "    samp {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem !important;\n",
       "    }\n",
       "\n",
       "    h1 {\n",
       "        font-size: 2rem !important;\n",
       "        font-weight: 500 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: uppercase !important;\n",
       "    }\n",
       "\n",
       "    h2 {\n",
       "        font-size: 1.8rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: none !important;\n",
       "    }\n",
       "\n",
       "    h3 {\n",
       "        font-size: 1.5rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        font-style: italic !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    h4,\n",
       "    h5,\n",
       "    h6 {\n",
       "        font-size: 1rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    .prompt {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem;\n",
       "        text-align: right;\n",
       "        line-height: 1.21429rem;\n",
       "    }\n",
       "\n",
       "    /* INTRO PAGE */\n",
       "\n",
       "    .toolbar_info,\n",
       "    .list-container {\n",
       "        ;\n",
       "    }\n",
       "    /* NOTEBOOK */\n",
       "\n",
       "    div#header-container {\n",
       "        display: none !important;\n",
       "    }\n",
       "\n",
       "    div#notebook {\n",
       "        border-top: none;\n",
       "        font-size: 1rem;\n",
       "    }\n",
       "\n",
       "    div.input_prompt {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .code_cell div.input_prompt:after,\n",
       "    div.output_prompt:after {\n",
       "        content: '\\25b6';\n",
       "    }\n",
       "\n",
       "    div.output_prompt {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    div.input_area {\n",
       "        border-radius: 0px;\n",
       "        border: 1px solid #d8d8d8;\n",
       "    }\n",
       "\n",
       "    div.output_area pre {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    div.output_subarea {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .rendered_html pre,\n",
       "    .rendered_html table,\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        border: 1px #828282 solid;\n",
       "        font-size: 0.75rem;\n",
       "        font-family: 'Menlo', monospace;\n",
       "    }\n",
       "\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        padding: 5px 10px;\n",
       "    }\n",
       "\n",
       "    .rendered_html th {\n",
       "        font-weight: normal;\n",
       "        background: #f8f8f8;\n",
       "    }\n",
       "\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "\n",
       "    div.output_html {\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    table.dataframe tr {\n",
       "        border: 1px #CCCCCC;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border-radius: 0px;\n",
       "    }\n",
       "\n",
       "    div.cell.edit_mode {\n",
       "        border-radius: 0px;\n",
       "        border: thin solid #CF5804;\n",
       "    }\n",
       "\n",
       "    span.ansiblue {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    span.ansigray {\n",
       "        color: #d8d8d8;\n",
       "    }\n",
       "\n",
       "    span.ansigreen {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    span.ansipurple {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    span.ansired {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    span.ansiyellow {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    div.output_stderr {\n",
       "        background-color: #D43132;\n",
       "    }\n",
       "\n",
       "    div.output_stderr pre {\n",
       "        color: #e8e8e8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython.CodeMirror {\n",
       "        background: #F8F8F8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython div.CodeMirror-selected {\n",
       "        background: #e8e8e8 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-gutters {\n",
       "        background: #F8F8F8;\n",
       "        border-right: 0px;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-linenumber {\n",
       "        color: #b8b8b8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-cursor {\n",
       "        border-left: 1px solid #585858 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-atom {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-number {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-property,\n",
       "    .cm-s-ipython span.cm-attribute {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        font-weight: normal;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-string {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-operator {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-builtin {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable-2 {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-def {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-error {\n",
       "        background: #FFBDBD;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-tag {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-link {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-matchingbracket {\n",
       "        text-decoration: underline;\n",
       "         !important;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', '..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(css_style='custom2.css', plot_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1670b75-dd14-4a20-afa3-5f0ffacbaa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2024-03-19\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.18\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "datasets         : 2.14.7\n",
      "pandas           : 2.2.0\n",
      "numpy            : 1.23.5\n",
      "torch            : 2.1.2\n",
      "peft             : 0.9.0\n",
      "faiss            : 1.7.2\n",
      "transformers     : 4.37.0\n",
      "pytorch_lightning: 2.1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import peft\n",
    "import faiss\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.optim import AdamW\n",
    "from torch.distributed import nn as dist_nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers.utils import PaddingStrategy\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "\n",
    "%watermark -a 'Ethen' -d -u -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad608a51-4868-4462-82c8-3edb8fcc4535",
   "metadata": {},
   "source": [
    "# Multilingual Sentence Embedding with LLM and PEFT LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec591267-051c-46be-a8c2-aa2de829d415",
   "metadata": {},
   "source": [
    "In this article, we'll be taking a look at training a multilingual sentence embedding with Large Language Model (LLM) and a parameter efficient fine tuning technique: LoRA (Low Rank Adaption)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff95dac-8993-4154-b88a-281a0892c82a",
   "metadata": {},
   "source": [
    "## LLM For Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b49ac9-98c1-4131-86d7-d9e716c16508",
   "metadata": {},
   "source": [
    "Large Language Model (LLM) with billion of parameters, fine-tuned to follow instructions have showcased remarkable capabilities on many NLP tasks. Consequently, there's a growing interest in harnessing these models as retrieval systems, such as LLAMA-2 [[7]](https://arxiv.org/abs/2310.08319), GPT [[10]](https://arxiv.org/abs/2201.10005), Mistral [[11]](https://arxiv.org/abs/2401.00368).\n",
    "\n",
    "RepLLaMA/RankLLaMA [[7]](https://arxiv.org/abs/2310.08319) leverages LLAMA-2-7B as backbone model for training retrieval and re-ranker model. Previous work on dense retriever models often uses bi-directional encoder model like BERT, taking the representation of prepended special [CLS] token or average pooling as sentence embedding. Given LLAMA is a uni-directional decoder only, an end of sentence token <\\s> is appended to serve as embedding.\n",
    "\n",
    "For addressing high GPU memory cost associated with fine tuning large models with contrastive learning, they leverage memory efficiency solutions such as LoRA, flash attention, and gradient checkpointing. The model is trained on 16 x 32G V100 GPUs with a batch size of 128, hard negatives from a blend of BFM25 and CoCondenser to ensure hard negatives are derived from both sparse and dense retrieval results.\n",
    "\n",
    "Apart from potent performance when evaluated on in-domain dataset MS MARCO and zero shot evaluation on BEIR benchmark suite, it also offers the advantage that modern LLM are often pre-trained with longer context window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b3c47-bc18-4cf4-b4e1-af8918cc62f0",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956b478-3295-43a4-8647-df4f2bd008b2",
   "metadata": {},
   "source": [
    "In modern transformer pre-trained model era, many application rely on fine tuning one large pre-trained model to multiple down stream applications. Given the higher associated cost with fine tuning, many sought to adapt only partial parameters, i.e. freezing base layers. LoRA (Low Rank Adaptation) [[9]](https://arxiv.org/abs/2106.09685) presents an alternative approach by representing the weight update with two low rank matrices.\n",
    "\n",
    "Quoting the LoRA paper: Given a weight matrix $W_0 \\in \\mathbb{R}^{d \\times d}$, we would constrain its update $W_0 + \\Delta W = W_0 + BA$, where $B \\in \\mathbb{R}^{d \\times r}$,  $B \\in \\mathbb{R}^{r \\times d}$. During training $W_0$ is frozen, while $A$ and $B$ contain trainable parameters. Both set of matrices would receiving the same input during forward pass: $W_0 x + \\alpha \\Delta W x = W_0 x + \\alpha BA x$, where $\\alpha$ is a scaling constant. At the beginning, $A$ is initialized with random Gaussian, and zero for $B$.\n",
    "\n",
    "<img src=\"imgs/lora.png\">\n",
    "\n",
    "\n",
    "Its advantages:\n",
    "\n",
    "- A pre-trained model can be shared, and use to build many small LoRA modules for different tasks.\n",
    "- Compared to full fine tuning, training becomes more efficien as it drastically reduces the number of trainable parameters. Lowering the hardware barrier as well as accelerating training cycle, especially when it comes to billion sized pre-trained models.\n",
    "- Its linear design allows us to merge LoRA's trainable matrices with the original frozen weights, effectively introducing zero additional inference latency compared to the original model.\n",
    "\n",
    "```python\n",
    "# simplified lora linear layer\n",
    "class LoraLinear(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        rank,\n",
    "        alpha,\n",
    "        bias=True,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.lora_A = nn.Parameters(torch.rand(in_features, rank))\n",
    "        self.lora_B = nn.Parameters(torch.zeros(rank, out_features))\n",
    "\n",
    "        # freeze the original linear layer's weight matrix\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_weights = x @ self.lora_A @ self.lora_B * self.alpha\n",
    "        return super().forward(x) + lora_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ad222-9ac9-430a-866a-5a7bc3896b36",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea4719-9a8c-4fbe-82bb-e36aeb87aa5e",
   "metadata": {},
   "source": [
    "We'll be utilizing the bloomz model family as our tokenizer/model. We have the flexibility to substitute it with any other Language Model Models (LLMs), we've opted for the bloomz model family for its multilingual capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759193ae-97e2-4b77-9d98-9306892fbdf7",
   "metadata": {},
   "source": [
    "### ESCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c5f68-a85c-4e0f-ae4e-a679b9997071",
   "metadata": {},
   "source": [
    "For our dataset, we taking inspiration from one of the examples from `peft` library's [[2]](https://huggingface.co/docs/peft/main/en/task_guides/semantic-similarity-lora) documentation. Specifically, we'll be using a small subset of ESCI e-commerce search query dataset that's conveniently available on huggingface dataset. The ESCI dataset [[3]](https://github.com/amazon-science/esci-data) [[8]](https://arxiv.org/abs/2206.06588), available in multiple languages including English, Japanese, and Spanish, consists of challenging search queries (such as those involving negations: \"energy bar without nuts\" or \"gluten-free biscuits\") paired with up to 40 search results, along with their ESCI (Exact, Substitute, Complement, Irrelevant) judgments. Our task at hand will be to train a model for retrieving similar products for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500699f4-ad42-4048-bf40-3d94076defe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/bigscience/bloomz-1b7\n",
    "model_name_or_path = \"bigscience/bloomz-1b7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad01da7f-c0d3-46af-ab23-f3265f8bc49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 839306/839306 [00:07<00:00, 106693.67 examples/s]\n",
      "Filter: 100%|██████████| 363402/363402 [00:03<00:00, 106600.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '!awnmower tires without rims', 'product_title': 'MaxAuto 2-Pack 13x5.00-6 2PLY Turf Mower Tractor Tire with Yellow Rim, (3\" Centered Hub, 3/4\" Bushings )', 'product_id': 'B08L3B9B9P', 'esci_label': 'E', 'split': 'train', 'relevance_label': 1, '__index_level_0__': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'product_title', 'product_id', 'esci_label', 'split', 'relevance_label', '__index_level_0__'],\n",
       "        num_rows: 658894\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query', 'product_title', 'product_id', 'esci_label', 'split', 'relevance_label', '__index_level_0__'],\n",
       "        num_rows: 286542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"smangrul/amazon_esci\"\n",
    "dataset_dict = load_dataset(dataset_name).filter(lambda example: example[\"relevance_label\"] == 1)\n",
    "print(dataset_dict[\"train\"][0])\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eabc4e5-53bf-4801-a1ff-1ea921aca0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSentenceEmbedding(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    tokenize raw text as well as padding while forming a batch for data loader.\n",
    "    Append eos token for downstream embedding representation.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: Optional[PreTrainedTokenizerBase] = None\n",
    "    max_seq_len_1: int = 512\n",
    "    max_seq_len_2: int = 512\n",
    "    id_field: str = \"__index_level_0__\"\n",
    "    text_field_1: str = \"query\"\n",
    "    text_field_2: str = \"product_title\"\n",
    "    process_tower: Optional[str] = None\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]], return_tensors=None) -> Dict[str, Any]:\n",
    "        # id could be a string column, and is also not part module's forward pass\n",
    "        # hence converting to torch tensor isn't needed\n",
    "        ids = [feature[self.id_field] for feature in features]\n",
    "\n",
    "        if self.process_tower == \"tower_1\":\n",
    "            formatted_text = [feature[self.text_field_1] + tokenizer.eos_token for feature in features]\n",
    "            tokenized_text_1 = self.tokenizer(\n",
    "                text=formatted_text,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_seq_len_1,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=self.return_tensors\n",
    "            )\n",
    "\n",
    "            batch = {\n",
    "                \"ids\": ids,\n",
    "                \"input_ids\": tokenized_text_1[\"input_ids\"],\n",
    "                \"attention_mask\": tokenized_text_1[\"attention_mask\"]\n",
    "            }\n",
    "        elif self.process_tower == \"tower_2\":\n",
    "            formatted_text = [feature[self.text_field_2] + tokenizer.eos_token for feature in features]\n",
    "            tokenized_text_2 = self.tokenizer(\n",
    "                text=formatted_text,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_seq_len_2,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=self.return_tensors\n",
    "            )\n",
    "\n",
    "            batch = {\n",
    "                \"ids\": ids,\n",
    "                \"input_ids\": tokenized_text_2[\"input_ids\"],\n",
    "                \"attention_mask\": tokenized_text_2[\"attention_mask\"]\n",
    "            }\n",
    "        else:            \n",
    "            formatted_text_1 = [feature[self.text_field_1] + tokenizer.eos_token for feature in features]\n",
    "            tokenized_text_1 = self.tokenizer(\n",
    "                text=formatted_text_1,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_seq_len_1,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=self.return_tensors\n",
    "            )\n",
    "\n",
    "            formatted_text_2 = [feature[self.text_field_2] + tokenizer.eos_token for feature in features]\n",
    "            tokenized_text_2 = self.tokenizer(\n",
    "                text=formatted_text_2,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_seq_len_2,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=self.return_tensors\n",
    "            )\n",
    "\n",
    "            batch = {\n",
    "                \"ids\": ids,\n",
    "                \"input_ids_1\": tokenized_text_1[\"input_ids\"],\n",
    "                \"input_ids_2\": tokenized_text_2[\"input_ids\"],\n",
    "                \"attention_mask_1\": tokenized_text_1[\"attention_mask\"],\n",
    "                \"attention_mask_2\": tokenized_text_2[\"attention_mask\"]\n",
    "            }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691b24e3-b1a7-485f-b323-52ca11b31424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [699034, 746719],\n",
       " 'input_ids_1': tensor([[ 24027,   2969,   8457,   2629, 170205,      2],\n",
       "         [ 84846,   6303,   5669,   1640,  15486,      2]]),\n",
       " 'input_ids_2': tensor([[     3,      3,      3,      3,      3,      3,      3,      3,   1980,\n",
       "            6844, 150996,    337,   3846,    375,   3548,  13281,  78211,     12,\n",
       "               2],\n",
       "         [ 57277,  98007,  64937,   3541,  49761,  84109, 115011,  18832,   2967,\n",
       "           86498,  67901,   2137,  18728,    530,   8557,  15486,     15,  21107,\n",
       "               2]]),\n",
       " 'attention_mask_1': tensor([[1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1]]),\n",
       " 'attention_mask_2': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSentenceEmbedding(tokenizer)\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "batch = next(iter(dataloader_train))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b7429-de63-47d2-ad39-228679c04db9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2035027-5fb4-4e76-9132-bf67c5afea0c",
   "metadata": {},
   "source": [
    "The following next code chunk defines a huggingface compatible `SentenceEmbeddingModel` for training retrieval model using contrastive learning. For actual LoRA experimentation, we'll directly leverage `peft` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0f91d3-23df-4123-9485-7d0492497d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbeddingModelConfig(PretrainedConfig):\n",
    "    model_type = \"sentence_embedding\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        normalize: bool = True,\n",
    "        cross_gpu_negatives: bool = False,\n",
    "        enable_gradient_checkpointing: bool = True,\n",
    "        peft_config = None\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.normalize = normalize\n",
    "        self.cross_gpu_negatives = cross_gpu_negatives and torch.cuda.device_count() > 1\n",
    "        self.enable_gradient_checkpointing = enable_gradient_checkpointing\n",
    "        self.peft_config = peft_config\n",
    "\n",
    "\n",
    "class SentenceEmbeddingModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    InfoNCE style contrastive loss sentence embedding.\n",
    "    Uses last token (eos) as embedding representation,\n",
    "    gradient checkpointing, LoRA for memory efficient training\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = SentenceEmbeddingModelConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        model = AutoModel.from_pretrained(config.model_name)\n",
    "        if config.enable_gradient_checkpointing:\n",
    "            model.gradient_checkpointing_enable(\n",
    "                gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    "            )\n",
    "\n",
    "        self.model = model\n",
    "        if config.peft_config:\n",
    "            self.model = get_peft_model(model, config.peft_config)\n",
    "            self.model.print_trainable_parameters()\n",
    "\n",
    "        self.loss = CLIPLoss(config.cross_gpu_negatives)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_1,\n",
    "        attention_mask_1,\n",
    "        input_ids_2,\n",
    "        attention_mask_2,\n",
    "    ):\n",
    "        embeddings_1 = self.encode(input_ids_1, attention_mask_1)\n",
    "        embeddings_2 = self.encode(input_ids_2, attention_mask_2)\n",
    "        loss = self.loss(embeddings_1, embeddings_2)\n",
    "        return loss, embeddings_1, embeddings_2\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        model_output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = last_token_pooling(model_output.last_hidden_state, attention_mask)\n",
    "        if self.config.normalize:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def last_token_pooling(last_hidden_states, attention_mask):\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "\n",
    "class CLIPLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Symmetric contrastive learning, a.k.a. CLIP loss or\n",
    "    Multiple Negative Ranking Loss that's mentioned in the sentence bert package.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - https://arxiv.org/abs/2103.00020\n",
    "    - https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cross_gpu_negatives: bool = True):\n",
    "        super().__init__()\n",
    "        self.cross_gpu_negatives = cross_gpu_negatives\n",
    "\n",
    "        # trainable temperature parameters\n",
    "        # This initial value is based on open clip\n",
    "        # https://github.com/mlfoundations/open_clip/blob/4b929357093bfbb0986b61cfa23776f1dc740370/src/open_clip/model.py\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, anchor_embedding, positive_embedding):\n",
    "        with torch.no_grad():\n",
    "            self.logit_scale.clamp_(0, math.log(100))\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        if self.cross_gpu_negatives:\n",
    "            anchor_embedding_all_gathered = torch.cat(dist_nn.all_gather(anchor_embedding), dim=0)\n",
    "            positive_embedding_all_gathered = torch.cat(dist_nn.all_gather(positive_embedding), dim=0)\n",
    "            anchor_scores = anchor_embedding @ positive_embedding_all_gathered.T * logit_scale\n",
    "            positive_scores = positive_embedding @ anchor_embedding_all_gathered.T * logit_scale\n",
    "            rank = dist.get_rank()\n",
    "        else:\n",
    "            anchor_scores = anchor_embedding @ positive_embedding.T * logit_scale\n",
    "            positive_scores = positive_embedding @ anchor_embedding.T * logit_scale\n",
    "            rank = 0\n",
    "\n",
    "        # Example a[i] should match with p[i]\n",
    "        batch_size = anchor_scores.size()[0]\n",
    "        labels = torch.arange(batch_size, device=anchor_scores.device, dtype=torch.long)\n",
    "        labels = labels + batch_size * rank\n",
    "        loss = (F.cross_entropy(anchor_scores, labels) + F.cross_entropy(positive_scores, labels)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9cc98-5552-4339-9800-51e7ce24bcd6",
   "metadata": {},
   "source": [
    "As part of our `LoraConfig`, we need to specify `target_modules`, which checks if the specified substring is in module's full name. LoRA can be applied to any module in our model, though the most common practice for transformer style model is applying to to attention layer's key, value, query matrices as well as its immediate feed forward layer. \n",
    "\n",
    "With our LoRA setup along with gradient checkpointing, we are able to train a 1.7B model using a single V100 GPU with micro batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c5a73b7-3c15-4cf9-9f7e-ca51529a0e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 1,728,700,416 || trainable%: 0.363941371319714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceEmbeddingModel(\n",
       "  (model): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoraModel(\n",
       "      (model): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 2048)\n",
       "        (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss): CLIPLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/peft/en/package_reference/lora\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    # check each model's corresponding module name,\n",
    "    # e.g. for BERT, target_modules=[\"key\", \"query\", \"value\"],\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],\n",
    "\n",
    "    # parameters that were not injected with LoRA are automatically\n",
    "    # frozen, if we wish to train them, specify them via\n",
    "    # modules_to_save\n",
    ")\n",
    "sentence_embedding_model_config = SentenceEmbeddingModelConfig(\n",
    "    model_name_or_path,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "sentence_embedding_model = SentenceEmbeddingModel(sentence_embedding_model_config)\n",
    "sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a79153-7c04-4a4e-a5f0-49ae6892f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = batch.pop(\"ids\")\n",
    "output = sentence_embedding_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968b58f5-0d95-4923-9386-70fa6b50594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbeddingLightningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, sentence_embedding_model: SentenceEmbeddingModel):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding_model = sentence_embedding_model\n",
    "\n",
    "        # huggingface auto model loads model in eval mode. Latest version of\n",
    "        # pytorch lightning no longer auto converts model to train mode during\n",
    "        # trainer fit stage, end user need to explicitly call them\n",
    "        # https://github.com/Lightning-AI/pytorch-lightning/issues/19467#issuecomment-1942741283\n",
    "        self.sentence_embedding_model.train()\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        return self.sentence_embedding_model(**batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids = batch.pop(\"ids\")\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        ids = batch.pop(\"ids\")\n",
    "        embeddings = self.sentence_embedding_model.encode(**batch)\n",
    "        prediction_output = {\"ids\": ids, \"embeddings\": embeddings}\n",
    "        return prediction_output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model = self.sentence_embedding_model\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            }\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=0.0001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f9017d6-77ea-433c-8dcc-ddf627530d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type                   | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | sentence_embedding_model | SentenceEmbeddingModel | 1.7 B \n",
      "--------------------------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "1.7 B     Non-trainable params\n",
      "1.7 B     Total params\n",
      "6,914.802 Total estimated model params size (MB)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`Trainer.fit` stopped: `max_steps=2000` reached.\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding_module = SentenceEmbeddingLightningModule(sentence_embedding_model)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=-1,\n",
    "    max_steps=2000,\n",
    "    precision=\"16-mixed\",\n",
    "    # note, we purpose-fully disabled the progress bar to prevent flooding our notebook's console\n",
    "    # in normal settings, we can/should definitely turn it on\n",
    "    enable_progress_bar=False,\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=2,\n",
    "    batch_size=64,\n",
    "    pin_memory=True,\n",
    ")\n",
    "trainer.fit(sentence_embedding_module, dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46afb2-81d7-408d-bbca-8cb14cd68b31",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53802c-c0cf-4d4f-9567-ddb5cbd07b4b",
   "metadata": {},
   "source": [
    "Evaluation process involves:\n",
    "\n",
    "- Generating embeddings for both distinct queries and products (corpus).\n",
    "- Retrieve top-k products using FAISS's flat index, i.e. exact cosine similarity.\n",
    "- Compute evaluation metrics, in this case recall@k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492330d6-f767-4118-bd34-de781ce1e8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceEmbeddingLightningModule(\n",
       "  (sentence_embedding_model): SentenceEmbeddingModel(\n",
       "    (model): BloomModel(\n",
       "      (word_embeddings): Embedding(250880, 2048)\n",
       "      (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (loss): CLIPLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the original model back for running inference\n",
    "sentence_embedding_module.sentence_embedding_model.model = sentence_embedding_module.sentence_embedding_model.model.merge_and_unload()\n",
    "sentence_embedding_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe45c6c5-5c50-4aee-94d6-f20a284210ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(predictions):\n",
    "    prediction_outputs = {\"ids\": [], \"embeddings\": []}\n",
    "    for prediction in predictions:\n",
    "        prediction_outputs[\"ids\"].extend(prediction[\"ids\"])\n",
    "        embeddings = [embedding for embedding in prediction[\"embeddings\"].cpu().numpy()]\n",
    "        prediction_outputs[\"embeddings\"].extend(embeddings)\n",
    "    \n",
    "    return pd.DataFrame(prediction_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76d851f0-9ee6-40f6-8e68-7c37b8c2900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_id</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>split</th>\n",
       "      <th>relevance_label</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!qscreen fence without holes</td>\n",
       "      <td>Zippity Outdoor Products ZP19026 Lightweight P...</td>\n",
       "      <td>B07DHX8YH2</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!qscreen fence without holes</td>\n",
       "      <td>ColourTree 4' x 50' Green Fence Privacy Screen...</td>\n",
       "      <td>B07DS1YCRZ</td>\n",
       "      <td>S</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!qscreen fence without holes</td>\n",
       "      <td>ColourTree 6' x 50' Black Fence Privacy Screen...</td>\n",
       "      <td>B07DS3J3MB</td>\n",
       "      <td>S</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!qscreen fence without holes</td>\n",
       "      <td>Sunnyglade 6 feet x 50 feet Privacy Screen Fen...</td>\n",
       "      <td>B07MFP4PPQ</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!qscreen fence without holes</td>\n",
       "      <td>Amgo 6' x 50' Black Fence Privacy Screen Winds...</td>\n",
       "      <td>B07R3TNQDM</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286537</th>\n",
       "      <td>香奈儿</td>\n",
       "      <td>Chânél Chance Eau Tendre Eau de Toilette Women...</td>\n",
       "      <td>B08181G6MP</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2614586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286538</th>\n",
       "      <td>香奈儿</td>\n",
       "      <td>Steve Madden Designer 15 Inch Carry on Suitcas...</td>\n",
       "      <td>B01HFH4DAI</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2614587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286539</th>\n",
       "      <td>香奈儿</td>\n",
       "      <td>CHANEL Le Lift Creme Yeux, Black, 0.5 Ounce</td>\n",
       "      <td>B00NIQGQAQ</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2614588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286540</th>\n",
       "      <td>香奈儿</td>\n",
       "      <td>Chanel Bleu de Chanel Eau de Parfum Spray for ...</td>\n",
       "      <td>B00NAGVL7W</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2614589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286541</th>\n",
       "      <td>香奈儿</td>\n",
       "      <td>Chânél No. 5 by Chânél Eau De Parfum Premiere ...</td>\n",
       "      <td>B081X6DRRT</td>\n",
       "      <td>E</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2614593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286542 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               query  \\\n",
       "0       !qscreen fence without holes   \n",
       "1       !qscreen fence without holes   \n",
       "2       !qscreen fence without holes   \n",
       "3       !qscreen fence without holes   \n",
       "4       !qscreen fence without holes   \n",
       "...                              ...   \n",
       "286537                           香奈儿   \n",
       "286538                           香奈儿   \n",
       "286539                           香奈儿   \n",
       "286540                           香奈儿   \n",
       "286541                           香奈儿   \n",
       "\n",
       "                                            product_title  product_id  \\\n",
       "0       Zippity Outdoor Products ZP19026 Lightweight P...  B07DHX8YH2   \n",
       "1       ColourTree 4' x 50' Green Fence Privacy Screen...  B07DS1YCRZ   \n",
       "2       ColourTree 6' x 50' Black Fence Privacy Screen...  B07DS3J3MB   \n",
       "3       Sunnyglade 6 feet x 50 feet Privacy Screen Fen...  B07MFP4PPQ   \n",
       "4       Amgo 6' x 50' Black Fence Privacy Screen Winds...  B07R3TNQDM   \n",
       "...                                                   ...         ...   \n",
       "286537  Chânél Chance Eau Tendre Eau de Toilette Women...  B08181G6MP   \n",
       "286538  Steve Madden Designer 15 Inch Carry on Suitcas...  B01HFH4DAI   \n",
       "286539        CHANEL Le Lift Creme Yeux, Black, 0.5 Ounce  B00NIQGQAQ   \n",
       "286540  Chanel Bleu de Chanel Eau de Parfum Spray for ...  B00NAGVL7W   \n",
       "286541  Chânél No. 5 by Chânél Eau De Parfum Premiere ...  B081X6DRRT   \n",
       "\n",
       "       esci_label split  relevance_label  __index_level_0__  \n",
       "0               E  test                1                 34  \n",
       "1               S  test                1                 35  \n",
       "2               S  test                1                 36  \n",
       "3               E  test                1                 39  \n",
       "4               E  test                1                 41  \n",
       "...           ...   ...              ...                ...  \n",
       "286537          E  test                1            2614586  \n",
       "286538          E  test                1            2614587  \n",
       "286539          E  test                1            2614588  \n",
       "286540          E  test                1            2614589  \n",
       "286541          E  test                1            2614593  \n",
       "\n",
       "[286542 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = dataset_dict[\"validation\"].to_pandas()\n",
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a669c599-1b13-411d-9d96-880704c21276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!qscreen fence without holes</td>\n",
       "      <td>[-0.015719872, 0.0092682745, 0.0032925562, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1 black natural hair dye without ammonia or p...</td>\n",
       "      <td>[0.007189093, 0.023743032, 0.015393866, 0.0183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1 rated resveratrol supplement without tea le...</td>\n",
       "      <td>[-0.016837992, 0.028490836, 0.004562847, 0.022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#10 envelopes without security tint</td>\n",
       "      <td>[-0.021525824, -0.038583543, 0.0010891705, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#10 standard no tint no window not self seal</td>\n",
       "      <td>[-0.0038119196, -0.0015254373, 0.00934174, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8951</th>\n",
       "      <td>zone mouthguard</td>\n",
       "      <td>[0.01643445, -0.015690504, -0.007845199, 0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8952</th>\n",
       "      <td>zoom groom for dogs</td>\n",
       "      <td>[-0.051384337, 0.0015823826, 0.0002485972, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8953</th>\n",
       "      <td>zozoville puzzle</td>\n",
       "      <td>[0.00043908256, -0.013274448, -0.010934159, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8954</th>\n",
       "      <td>سماعة gaming pc</td>\n",
       "      <td>[-0.020317372, -0.004169517, -0.0020055668, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8955</th>\n",
       "      <td>香奈儿</td>\n",
       "      <td>[-0.0122507345, -0.0038623791, 0.0058575706, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8956 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ids  \\\n",
       "0                          !qscreen fence without holes   \n",
       "1     #1 black natural hair dye without ammonia or p...   \n",
       "2     #1 rated resveratrol supplement without tea le...   \n",
       "3                   #10 envelopes without security tint   \n",
       "4          #10 standard no tint no window not self seal   \n",
       "...                                                 ...   \n",
       "8951                                    zone mouthguard   \n",
       "8952                                zoom groom for dogs   \n",
       "8953                                   zozoville puzzle   \n",
       "8954                                    سماعة gaming pc   \n",
       "8955                                                香奈儿   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [-0.015719872, 0.0092682745, 0.0032925562, 0.0...  \n",
       "1     [0.007189093, 0.023743032, 0.015393866, 0.0183...  \n",
       "2     [-0.016837992, 0.028490836, 0.004562847, 0.022...  \n",
       "3     [-0.021525824, -0.038583543, 0.0010891705, -0....  \n",
       "4     [-0.0038119196, -0.0015254373, 0.00934174, -0....  \n",
       "...                                                 ...  \n",
       "8951  [0.01643445, -0.015690504, -0.007845199, 0.033...  \n",
       "8952  [-0.051384337, 0.0015823826, 0.0002485972, -0....  \n",
       "8953  [0.00043908256, -0.013274448, -0.010934159, -0...  \n",
       "8954  [-0.020317372, -0.004169517, -0.0020055668, 0....  \n",
       "8955  [-0.0122507345, -0.0038623791, 0.0058575706, 0...  \n",
       "\n",
       "[8956 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query = df_validation[[\"query\"]].drop_duplicates().reset_index(drop=True)\n",
    "dataset_query = datasets.Dataset.from_pandas(df_query)\n",
    "data_collator = DataCollatorForSentenceEmbedding(\n",
    "    tokenizer,\n",
    "    process_tower=\"tower_1\",\n",
    "    id_field=\"query\"\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset_query,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=64,\n",
    "    pin_memory=True,\n",
    "    num_workers=2\n",
    ")\n",
    "predictions = trainer.predict(sentence_embedding_module, dataloader)\n",
    "df_query = postprocess_predictions(predictions)\n",
    "df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d43a60a-537a-4c1c-881c-f362edd1a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B07DHX8YH2</td>\n",
       "      <td>[-0.0298094, 0.0066097863, -0.007526012, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B07DS1YCRZ</td>\n",
       "      <td>[-0.00046652087, -0.012415803, 0.008917227, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07DS3J3MB</td>\n",
       "      <td>[-0.0028801567, -0.009671217, 0.011117947, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07MFP4PPQ</td>\n",
       "      <td>[-0.02145726, -0.016728751, 0.0012048857, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07R3TNQDM</td>\n",
       "      <td>[-0.020977724, -0.01857435, 0.010523445, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132773</th>\n",
       "      <td>B01E7KBXWC</td>\n",
       "      <td>[-0.016616581, -0.004221971, 0.009728117, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132774</th>\n",
       "      <td>B08181G6MP</td>\n",
       "      <td>[-0.016768515, 0.018336102, 0.010495669, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132775</th>\n",
       "      <td>B01HFH4DAI</td>\n",
       "      <td>[0.01899822, 0.010939811, -0.0010054795, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132776</th>\n",
       "      <td>B00NIQGQAQ</td>\n",
       "      <td>[-0.053966485, 0.004088956, 0.00047079526, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132777</th>\n",
       "      <td>B081X6DRRT</td>\n",
       "      <td>[-0.020010458, 0.0047987755, 0.003917635, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132778 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ids                                         embeddings\n",
       "0       B07DHX8YH2  [-0.0298094, 0.0066097863, -0.007526012, -0.00...\n",
       "1       B07DS1YCRZ  [-0.00046652087, -0.012415803, 0.008917227, -0...\n",
       "2       B07DS3J3MB  [-0.0028801567, -0.009671217, 0.011117947, -0....\n",
       "3       B07MFP4PPQ  [-0.02145726, -0.016728751, 0.0012048857, -0.0...\n",
       "4       B07R3TNQDM  [-0.020977724, -0.01857435, 0.010523445, -0.02...\n",
       "...            ...                                                ...\n",
       "132773  B01E7KBXWC  [-0.016616581, -0.004221971, 0.009728117, -0.0...\n",
       "132774  B08181G6MP  [-0.016768515, 0.018336102, 0.010495669, -0.01...\n",
       "132775  B01HFH4DAI  [0.01899822, 0.010939811, -0.0010054795, -0.01...\n",
       "132776  B00NIQGQAQ  [-0.053966485, 0.004088956, 0.00047079526, 0.0...\n",
       "132777  B081X6DRRT  [-0.020010458, 0.0047987755, 0.003917635, -0.0...\n",
       "\n",
       "[132778 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product = df_validation[[\"product_id\", \"product_title\"]].drop_duplicates().reset_index(drop=True)\n",
    "dataset_product = datasets.Dataset.from_pandas(df_product)\n",
    "data_collator = DataCollatorForSentenceEmbedding(\n",
    "    tokenizer,\n",
    "    process_tower=\"tower_2\",\n",
    "    id_field=\"product_id\"\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset_product,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=64,\n",
    "    pin_memory=True,\n",
    "    num_workers=2\n",
    ")\n",
    "predictions = trainer.predict(sentence_embedding_module, dataloader)\n",
    "df_corpus = postprocess_predictions(predictions)\n",
    "df_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fc02162-3e08-4094-80b3-f76083d31134",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ids = df_corpus[\"ids\"].tolist()\n",
    "index_embeddings = np.vstack(df_corpus[\"embeddings\"]).astype(np.float32)\n",
    "query_embeddings = np.vstack(df_query[\"embeddings\"]).astype(np.float32)\n",
    "dim = index_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947c7532-fe7b-46ec-a846-73e45aaee4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[129760,     10, 126665, ..., 129761, 130345,     11],\n",
       "       [    22,     43,     36, ...,  31539,     40,     18],\n",
       "       [    61,     79,     51, ...,     48,     76,     59],\n",
       "       ...,\n",
       "       [132754,   9639, 132755, ..., 132757,  98596, 106510],\n",
       "       [ 92659,  54626,  76087, ...,  14052,  14060,  61385],\n",
       "       [132772, 113343,  93151, ..., 113356, 132052, 132777]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk = 10\n",
    "knn_index = faiss.IndexFlatIP(dim)\n",
    "knn_index = faiss.index_cpu_to_all_gpus(knn_index)\n",
    "knn_index.add(index_embeddings)\n",
    "knn_scores, knn_indices = knn_index.search(query_embeddings, k=topk)\n",
    "knn_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8bb2542-4695-418f-afbe-b9fb4d3cbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert knn retrieval result to {query -> [list of knn retrieved corpus id]}\n",
    "knn_dict = {}\n",
    "for query, knn_indices_per_row in zip(df_query[\"ids\"], knn_indices):\n",
    "    corpus_indices_per_row = [index_ids[index] for index in knn_indices_per_row]\n",
    "    knn_dict[query] = corpus_indices_per_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a34d72-abef-4e07-b009-e89d3d2810c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert validation dataset to {query -> [list of ground truth corpus id]}\n",
    "eval_dict = {}\n",
    "for query, product_id in zip(df_validation[\"query\"], df_validation[\"product_id\"]):\n",
    "    if query not in eval_dict:\n",
    "        eval_dict[query] = [product_id]\n",
    "    else:\n",
    "        eval_dict[query].append(product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3d9bc2e-35fa-4367-908f-b880ed4396e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    knn_dict,\n",
    "    eval_dict,\n",
    "    top_k: int\n",
    "):\n",
    "    recalls = []\n",
    "    for query, knn_results in knn_dict.items():\n",
    "        knn_set = set(knn_results)\n",
    "        eval_set = set(eval_dict[query])\n",
    "        numerator = len(knn_set.intersection(eval_set))\n",
    "        denominator = min(len(eval_set), top_k)\n",
    "        recall = numerator / denominator\n",
    "        recalls.append(recall)\n",
    "\n",
    "    avg_recall = np.mean(recalls)\n",
    "    return avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d5a418c-0425-4ca3-b984-a204aa9fb9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3928341911425877"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(knn_dict, eval_dict, topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0ab5c-ef50-4967-90c3-e6eec0e34360",
   "metadata": {},
   "source": [
    "We conclude our article by offering some guidance when training with LoRA as well as decoder based retrieval models.\n",
    "\n",
    "LoRA:\n",
    "\n",
    "- The most critical LoRA hyperparameter is how many LoRA adapters are used in total and LoRA on all linear transformer block layers are required to match full fine tuning's performance. Other parameters such as projection dimension $r$ doesn't affect performance much. i.e. It's more preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.\n",
    "- When training with LoRA a lower learning rate as well as more steps might be required for matching full fine tuning's performance.\n",
    "- The effective of LoRA might be task dependent. Compared to full fine tuning, LoRA might stumble when ecountering more challenging tasks such as mathematical reasoning [[5]](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2).\n",
    "\n",
    "Personally, LoRA feels very much akin to matrix factorization, factorization machines family of methods with a twist.\n",
    "\n",
    "Decoder retrieval models:\n",
    "\n",
    "- Exploring LLMs' usage in embedding have garnered quite some interest with good reason, e.g. Improving text embeddings with LLMs [[11]](https://arxiv.org/abs/2401.00368) showed that using LLMs (Mistral 7B) as an initial backbone using synthetic data along with some moderate amount of labeled text pairs is sufficient, foregoing the need for large amounts of text pairs to obtain high quality embeddings.\n",
    "- Keep in mind that apart from performance, there's also the cost of operating these large LLMs for embedding use case. This is from a inference speed perspective as well as storage (billion parameter scale LLM typically involves generating a larger embedding hidden dimension, 2048+)[[6]](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dba795-ec75-4eb3-9148-c20ba9de82c6",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6ce27-9cab-477a-9d7b-a5f15ea89c03",
   "metadata": {},
   "source": [
    "- [[1]](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) PEFT Documentation: LoRA\n",
    "- [[2]](https://huggingface.co/docs/peft/main/en/task_guides/semantic-similarity-lora) PEFT Documentation: LoRA for semantic similarity tasks\n",
    "- [[3]](https://github.com/amazon-science/esci-data) Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search\n",
    "- [[4]](https://lightning.ai/lightning-ai/studios/code-lora-from-scratch) LoRA From Scratch – Implement Low-Rank Adaptation for LLMs in PyTorch\n",
    "- [[5]](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2\n",
    "- [[6]](https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9) OpenAI GPT-3 Text Embeddings - Really a new state-of-the-art in dense text embeddings?\n",
    "- [[7]](https://arxiv.org/abs/2310.08319) Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin - Fine-Tuning LLaMA for Multi-Stage Text Retrieval (2023)\n",
    "- [[8]](https://arxiv.org/abs/2206.06588) Chandan K. Reddy, Lluís Màrquez, Fran Valero, Nikhil Rao, Hugo Zaragoza, Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, Karthik Subbian - Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search (2022)\n",
    "- [[9]](https://arxiv.org/abs/2106.09685) Edward J. Hu, Yelong Shen, et al. - LoRA: Low-Rank Adaptation of Large Language Models (2021)\n",
    "- [[10]](https://arxiv.org/abs/2201.10005) Arvind Neelakantan, Tao Xu, et al. - Text and Code Embeddings by Contrastive Pre-Training (2022)\n",
    "- [[11]](https://arxiv.org/abs/2401.00368) Liang Wang, Nan Yang, Furu Wei, et al. - Improving Text Embeddings with Large Language Models (2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
