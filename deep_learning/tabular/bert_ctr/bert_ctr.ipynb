{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fb9033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    html {\n",
       "        font-size: 18px !important;\n",
       "    }\n",
       "\n",
       "    body {\n",
       "        background-color: #FFF !important;\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    body .notebook-app {\n",
       "        background-color: #FFF !important;\n",
       "    }\n",
       "\n",
       "    #header {\n",
       "        box-shadow: none !important;\n",
       "    }\n",
       "\n",
       "    #notebook {\n",
       "        padding-top: 0px;\n",
       "    }\n",
       "\n",
       "    #notebook-container {\n",
       "        box-shadow: none;\n",
       "        -webkit-box-shadow: none;\n",
       "        padding: 10px;\n",
       "    }\n",
       "\n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border: 1px dashed #CCCCCC;\n",
       "    }\n",
       "\n",
       "    .edit_mode div.cell.selected {\n",
       "        border: 1px dashed #828282;\n",
       "    }\n",
       "\n",
       "    div.output_wrapper {\n",
       "        margin-top: 8px;\n",
       "    }\n",
       "\n",
       "    a {\n",
       "        color: #383838;\n",
       "    }\n",
       "\n",
       "    code,\n",
       "    kbd,\n",
       "    pre,\n",
       "    samp {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem !important;\n",
       "    }\n",
       "\n",
       "    h1 {\n",
       "        font-size: 2rem !important;\n",
       "        font-weight: 500 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: uppercase !important;\n",
       "    }\n",
       "\n",
       "    h2 {\n",
       "        font-size: 1.8rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        letter-spacing: 3px !important;\n",
       "        text-transform: none !important;\n",
       "    }\n",
       "\n",
       "    h3 {\n",
       "        font-size: 1.5rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        font-style: italic !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    h4,\n",
       "    h5,\n",
       "    h6 {\n",
       "        font-size: 1rem !important;\n",
       "        font-weight: 400 !important;\n",
       "        display: block !important;\n",
       "    }\n",
       "\n",
       "    .prompt {\n",
       "        font-family: 'Menlo', monospace !important;\n",
       "        font-size: 0.75rem;\n",
       "        text-align: right;\n",
       "        line-height: 1.21429rem;\n",
       "    }\n",
       "\n",
       "    /* INTRO PAGE */\n",
       "\n",
       "    .toolbar_info,\n",
       "    .list-container {\n",
       "        ;\n",
       "    }\n",
       "    /* NOTEBOOK */\n",
       "\n",
       "    div#header-container {\n",
       "        display: none !important;\n",
       "    }\n",
       "\n",
       "    div#notebook {\n",
       "        border-top: none;\n",
       "        font-size: 1rem;\n",
       "    }\n",
       "\n",
       "    div.input_prompt {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .code_cell div.input_prompt:after,\n",
       "    div.output_prompt:after {\n",
       "        content: '\\25b6';\n",
       "    }\n",
       "\n",
       "    div.output_prompt {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    div.input_area {\n",
       "        border-radius: 0px;\n",
       "        border: 1px solid #d8d8d8;\n",
       "    }\n",
       "\n",
       "    div.output_area pre {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    div.output_subarea {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .rendered_html pre,\n",
       "    .rendered_html table,\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        border: 1px #828282 solid;\n",
       "        font-size: 0.75rem;\n",
       "        font-family: 'Menlo', monospace;\n",
       "    }\n",
       "\n",
       "    .rendered_html th,\n",
       "    .rendered_html tr,\n",
       "    .rendered_html td {\n",
       "        padding: 5px 10px;\n",
       "    }\n",
       "\n",
       "    .rendered_html th {\n",
       "        font-weight: normal;\n",
       "        background: #f8f8f8;\n",
       "    }\n",
       "\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "\n",
       "    div.output_html {\n",
       "        font-weight: 1rem;\n",
       "        font-family: 'Source Sans Pro', \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
       "    }\n",
       "\n",
       "    table.dataframe tr {\n",
       "        border: 1px #CCCCCC;\n",
       "    }\n",
       "\n",
       "    div.cell.selected {\n",
       "        border-radius: 0px;\n",
       "    }\n",
       "\n",
       "    div.cell.edit_mode {\n",
       "        border-radius: 0px;\n",
       "        border: thin solid #CF5804;\n",
       "    }\n",
       "\n",
       "    span.ansiblue {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    span.ansigray {\n",
       "        color: #d8d8d8;\n",
       "    }\n",
       "\n",
       "    span.ansigreen {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    span.ansipurple {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    span.ansired {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    span.ansiyellow {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    div.output_stderr {\n",
       "        background-color: #D43132;\n",
       "    }\n",
       "\n",
       "    div.output_stderr pre {\n",
       "        color: #e8e8e8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython.CodeMirror {\n",
       "        background: #F8F8F8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython div.CodeMirror-selected {\n",
       "        background: #e8e8e8 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-gutters {\n",
       "        background: #F8F8F8;\n",
       "        border-right: 0px;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-linenumber {\n",
       "        color: #b8b8b8;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-cursor {\n",
       "        border-left: 1px solid #585858 !important;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-atom {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-number {\n",
       "        color: #C74483;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-property,\n",
       "    .cm-s-ipython span.cm-attribute {\n",
       "        color: #688A0A;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        font-weight: normal;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-string {\n",
       "        color: #D9AA00;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-operator {\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-builtin {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-variable-2 {\n",
       "        color: #2B88D9;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-def {\n",
       "        color: #00A397;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-error {\n",
       "        background: #FFBDBD;\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-tag {\n",
       "        color: #D43132;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-link {\n",
       "        color: #975DDE;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython .CodeMirror-matchingbracket {\n",
       "        text-decoration: underline;\n",
       "         !important;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading notebook's format\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', '..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(css_style='custom2.css', plot_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f79036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2023-10-01\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "transformers: 4.31.0\n",
      "datasets    : 2.14.4\n",
      "torch       : 2.0.1\n",
      "numpy       : 1.23.2\n",
      "pandas      : 2.0.1\n",
      "sklearn     : 1.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as metrics\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    disable_progress_bar\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%watermark -a 'Ethen' -d -v -u -p transformers,datasets,torch,numpy,pandas,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091319c4",
   "metadata": {},
   "source": [
    "# BERT CTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a0706-892c-42c4-8373-5a4f63cadb3f",
   "metadata": {},
   "source": [
    "In many real world applications, our dataset is often times multi-modal [[3]](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4). Where there are textual information, structured/tabular data, or even data represented in image, video, audio format. In this article, we'll look at the problem of multimodal fusion, where our goal is to jointly model information from two or more modalities to make a prediction, specifically text and structured/tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f72a21-d0d3-4e17-9396-24acc4e7eac6",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab273b13-5894-4008-ab27-f6fe23874652",
   "metadata": {},
   "source": [
    "KDD 2012 Track 2 [[1]](https://www.kaggle.com/competitions/kddcup2012-track2/overview) is focused on predicting click through rate (CTR). This task involves not only textual data related to search query and ad, but also includes additional structured/tabular data such as user's demographic information gender/age, etc.\n",
    "\n",
    "Note, while this dataset we're using is slightly more representative of real world application, it unfortunately still has some limitations. The most notable one being, for anonymity reasons, this dataset provides tokens represented as hash values instead of actual raw text. This entails we can't directly leverage pre-trained tokenizer/model.\n",
    "\n",
    "Another dataset with similar characteristics is Baidu-ULTR [[2]](https://arxiv.org/abs/2207.03051). This dataset is advantageous in terms of its larger scale: featuring 1.2B search sessions, and its richness: offering diverse display information such as position, display height, and so on for studying un-biased learning to rank. As well as diversified user behavior, including click, skip, dwell time for exploring multi-task learning. Though, again, it also has similar limitation where raw text is provided in a hashed id manner.\n",
    "\n",
    "We'll be working with a sampled version of the original raw dataset. Readers can refer to [kdd2012_track2_preprocess_data.py](https://github.com/ethen8181/machine-learning/blob/master/deep_learning/tabular/bert_ctr/kdd2012_track2_preprocess_data.py) for a sample pre-processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e7c2ed-5ba4-4682-a06d-fc8980faf24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (900000, 17)\n",
      "test shape:  (100000, 17)\n",
      "label distribution:  label\n",
      "0    853036\n",
      "1     46964\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click</th>\n",
       "      <th>impression</th>\n",
       "      <th>display_url</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>advertiser_id</th>\n",
       "      <th>depth</th>\n",
       "      <th>position</th>\n",
       "      <th>query_id</th>\n",
       "      <th>keyword_id</th>\n",
       "      <th>title_id</th>\n",
       "      <th>description_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>tokenized_query</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5511132461021800102</td>\n",
       "      <td>20589102</td>\n",
       "      <td>572.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>666</td>\n",
       "      <td>476</td>\n",
       "      <td>152</td>\n",
       "      <td>3927.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[141907, 78371, 0, 64251, 69187, 23351, 110163...</td>\n",
       "      <td>[60776, 78371, 45372]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12057878999086460853</td>\n",
       "      <td>20163506</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>101369</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>3927.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[97424, 0, 74849, 127933, 62049, 142426]</td>\n",
       "      <td>[121115, 74849]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>781547179694832263</td>\n",
       "      <td>20886416</td>\n",
       "      <td>3015.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>10186269</td>\n",
       "      <td>1441</td>\n",
       "      <td>132250</td>\n",
       "      <td>33337</td>\n",
       "      <td>3927.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[63426, 65474, 49687, 113498, 61371, 102713, 4...</td>\n",
       "      <td>[122710, 65474, 26556]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14340390157469404125</td>\n",
       "      <td>3110734</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>64</td>\n",
       "      <td>121</td>\n",
       "      <td>170</td>\n",
       "      <td>3927.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[105048, 115914, 66509, 48803]</td>\n",
       "      <td>[66509, 71597, 119309, 67224, 129706]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5468727571223080485</td>\n",
       "      <td>20003716</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1348</td>\n",
       "      <td>1111</td>\n",
       "      <td>18547</td>\n",
       "      <td>23993</td>\n",
       "      <td>3605.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[119985, 1, 24536, 69187, 124848, 105787, 74848]</td>\n",
       "      <td>[119985]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click  impression           display_url     ad_id  advertiser_id  depth   \n",
       "0      0           1   5511132461021800102  20589102          572.0    0.0  \\\n",
       "1      0           1  12057878999086460853  20163506         1485.0    0.0   \n",
       "2      0           1    781547179694832263  20886416         3015.0    0.5   \n",
       "3      0           1  14340390157469404125   3110734         1224.0    0.0   \n",
       "4      0           1   5468727571223080485  20003716          102.0    0.5   \n",
       "\n",
       "   position  query_id  keyword_id  title_id  description_id  user_id  label   \n",
       "0         1       226         666       476             152   3927.0      0  \\\n",
       "1         1    101369           9        18              27   3927.0      0   \n",
       "2         2  10186269        1441    132250           33337   3927.0      0   \n",
       "3         1       284          64       121             170   3927.0      0   \n",
       "4         2      1348        1111     18547           23993   3605.0      0   \n",
       "\n",
       "                                     tokenized_title   \n",
       "0  [141907, 78371, 0, 64251, 69187, 23351, 110163...  \\\n",
       "1           [97424, 0, 74849, 127933, 62049, 142426]   \n",
       "2  [63426, 65474, 49687, 113498, 61371, 102713, 4...   \n",
       "3                     [105048, 115914, 66509, 48803]   \n",
       "4   [119985, 1, 24536, 69187, 124848, 105787, 74848]   \n",
       "\n",
       "                         tokenized_query  gender  age  \n",
       "0                  [60776, 78371, 45372]     2.0  6.0  \n",
       "1                        [121115, 74849]     2.0  1.0  \n",
       "2                 [122710, 65474, 26556]     2.0  5.0  \n",
       "3  [66509, 71597, 119309, 67224, 129706]     2.0  3.0  \n",
       "4                               [119985]     2.0  5.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_parquet(\"track2_processed_train.parquet\")\n",
    "df_test = pd.read_parquet(\"track2_processed_test.parquet\")\n",
    "print(\"train shape: \", df_train.shape)\n",
    "print(\"test shape: \", df_test.shape)\n",
    "print(\"label distribution: \", df_train[\"label\"].value_counts())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdda3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['click', 'impression', 'display_url', 'ad_id', 'advertiser_id', 'depth', 'position', 'query_id', 'keyword_id', 'title_id', 'description_id', 'user_id', 'label', 'tokenized_title', 'tokenized_query', 'gender', 'age'],\n",
      "    num_rows: 900000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'click': 0,\n",
       " 'impression': 1,\n",
       " 'display_url': 5511132461021800102,\n",
       " 'ad_id': 20589102,\n",
       " 'advertiser_id': 572.0,\n",
       " 'depth': 0.0,\n",
       " 'position': 1,\n",
       " 'query_id': 226,\n",
       " 'keyword_id': 666,\n",
       " 'title_id': 476,\n",
       " 'description_id': 152,\n",
       " 'user_id': 3927.0,\n",
       " 'label': 0,\n",
       " 'tokenized_title': [141907, 78371, 0, 64251, 69187, 23351, 110163, 74848],\n",
       " 'tokenized_query': [60776, 78371, 45372],\n",
       " 'gender': 2.0,\n",
       " 'age': 6.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "print(dataset_train)\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b6e7c-1cf5-46e1-9935-f9615ffedf1e",
   "metadata": {},
   "source": [
    "These subsequent function/class for using deep learning on tabular data closely follows the ones introduced in Deep Learning for Tabular Data - PyTorch. [[nbviewer](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/deep_learning/tabular/deep_learning_tabular.ipynb)][[html](http://ethen8181.github.io/machine-learning/deep_learning/tabular/deep_learning_tabular.html)]\n",
    "\n",
    "We'll specify a config mapping for our input features, which will be utilized in both our batch collate function as well as the model itself. This config mapping stores the features we want to use as keys, and different value/enum specifying whether the field is of text/token, numerical or categorical type. Which inform our pipeline about the embedding size required for a categorical type, how many numerical fields are there to initiate the dense/feed forward layers, whether to apply tokenization/padding for text/token type fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670acf08-ee32-412d-90df-141c85feb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a distil bert style architecture, with 2 layers, these\n",
    "# will be trained from scratch along with the CTR application\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "n_layers = 2\n",
    "# vocabulary size can be obtained from the data preprocessing script\n",
    "pad_token_id = 148597\n",
    "vocab_size = 148598\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131e3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in ordinal_encoder.categories_:\n",
    "#     print(len(category))\n",
    "features_config = {\n",
    "    \"tokenized_title\": {\n",
    "        \"dtype\": \"token_id\"\n",
    "    },\n",
    "    \"tokenized_query\": {\n",
    "        \"dtype\": \"token_id\"\n",
    "    },\n",
    "    \"depth\": {\n",
    "        \"dtype\": \"numerical\",\n",
    "    },\n",
    "    \"gender\": {\n",
    "        \"dtype\": \"categorical\",\n",
    "        \"vocab_size\": 4,\n",
    "        \"embedding_size\": 4\n",
    "    },\n",
    "    \"age\": {\n",
    "        \"dtype\": \"categorical\",\n",
    "        \"vocab_size\": 7,\n",
    "        \"embedding_size\": 4\n",
    "    },\n",
    "    \"advertiser_id\": {\n",
    "        \"dtype\": \"categorical\",\n",
    "        \"vocab_size\": 12193,\n",
    "        \"embedding_size\": 32\n",
    "    },\n",
    "    \"user_id\": {\n",
    "        \"dtype\": \"categorical\",\n",
    "        \"vocab_size\": 202547,\n",
    "        \"embedding_size\": 64\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def text_tabular_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Use in conjunction with Dataloader's collate_fn for text and tabular data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch : dict\n",
    "        Dictionary with 4 primary keys: input_ids, attention_mask, tabular_inputs, and labels.\n",
    "        Tabular inputs is a nested field, where each element is a feature_name -> float tensor\n",
    "        mapping. e.g.\n",
    "        {\n",
    "            'input_ids': tensor([[116438,  65110]]),\n",
    "            'attention_mask': tensor([[1,  1]]),\n",
    "            'tabular_inputs': {'I1': tensor([0., 0.]), 'C1': tensor([ 888., 1313.])},\n",
    "            'labels': tensor([0, 0])\n",
    "        }\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    labels = []\n",
    "    texts = []\n",
    "    token_ids = []\n",
    "    tabular_inputs = {}\n",
    "    for example in batch:\n",
    "        label = example[\"label\"]\n",
    "        labels.append(label)\n",
    "\n",
    "        text = \"\"\n",
    "        token_id = []\n",
    "        for name, config in features_config.items():\n",
    "            dtype = config[\"dtype\"]\n",
    "            feature = example[name]\n",
    "            if dtype == \"text\":\n",
    "                text += feature\n",
    "            elif dtype == \"token_id\":\n",
    "                token_id += feature\n",
    "            else:\n",
    "                if name not in tabular_inputs:\n",
    "                    tabular_inputs[name] = [feature]\n",
    "                else:\n",
    "                    tabular_inputs[name].append(feature)\n",
    "\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "        if token_id:\n",
    "            token_id = torch.LongTensor(token_id)\n",
    "            token_ids.append(token_id)\n",
    "\n",
    "    for name in tabular_inputs:\n",
    "        tabular_inputs[name] = torch.FloatTensor(tabular_inputs[name])\n",
    "\n",
    "    if texts:\n",
    "        tokenized = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "        output[\"input_ids\"] = tokenized[\"input_ids\"]\n",
    "        output[\"attention_mask\"] = tokenized[\"attention_mask\"]\n",
    "\n",
    "    if token_ids:\n",
    "        input_ids = pad_sequence(token_ids, batch_first=True, padding_value=pad_token_id)\n",
    "        attention_mask = (input_ids != pad_token_id).type(torch.int)\n",
    "        output[\"input_ids\"] = input_ids\n",
    "        output[\"attention_mask\"] = attention_mask\n",
    "\n",
    "    output[\"tabular_inputs\"] = tabular_inputs\n",
    "    output[\"labels\"] = torch.LongTensor(labels)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3f1aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[141907,  78371,      0,  64251,  69187,  23351, 110163,  74848,  60776,\n",
       "           78371,  45372],\n",
       "         [ 97424,      0,  74849, 127933,  62049, 142426, 121115,  74849, 148597,\n",
       "          148597, 148597]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], dtype=torch.int32),\n",
       " 'tabular_inputs': {'depth': tensor([0., 0.]),\n",
       "  'gender': tensor([2., 2.]),\n",
       "  'age': tensor([6., 1.]),\n",
       "  'advertiser_id': tensor([ 572., 1485.]),\n",
       "  'user_id': tensor([3927., 3927.])},\n",
       " 'labels': tensor([0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = DataLoader(dataset_train, batch_size=2, collate_fn=text_tabular_collate_fn)\n",
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16f01b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952971cc",
   "metadata": {},
   "source": [
    "The model architecture we'll be implementing looks something along the lines of:\n",
    "\n",
    "<img src=\"imgs/bert_ctr.png\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "1. Feeding textual input through BERT style transformer block, converting categorical features into a low dimensonal embedding, packing all of our numerical features together.\n",
    "2. The output from 3 different field groups are concatenated together before feeding them into subsequent feed forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd10fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_layers(input_dim: int, mlp_config):\n",
    "    \"\"\"\n",
    "    Construct MLP, a.k.a. Feed forward layers based on input config.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : \n",
    "        Input dimension for the first layer.\n",
    "\n",
    "    mlp_config : list of dictionary with mlp spec.\n",
    "        An example is shown below, the only mandatory parameter is hidden size.\n",
    "        ```\n",
    "        [\n",
    "            {\n",
    "                \"hidden_size\": 1024,\n",
    "                \"dropout_p\": 0.1,\n",
    "                \"activation_function\": \"ReLU\",\n",
    "                \"activation_function_kwargs\": {},\n",
    "                \"normalization_function\": \"LayerNorm\"\n",
    "                \"normalization_function_kwargs\": {\"eps\": 1e-05}\n",
    "            }\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Sequential :\n",
    "        Sequential layer converted from input mlp_config. If mlp_config\n",
    "        is None, then this returned value will also be None.\n",
    "\n",
    "    current_dim :\n",
    "        Dimension for the last layer.\n",
    "    \"\"\"\n",
    "    if mlp_config is None:\n",
    "        return None, input_dim\n",
    "\n",
    "    layers = []\n",
    "    current_dim = input_dim\n",
    "    for config in mlp_config:\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        dropout_p = config.get(\"dropout_p\", 0.0)\n",
    "        activation_function = config.get(\"activation_function\")\n",
    "        activation_function_kwargs = config.get(\"activation_function_kwargs\", {})\n",
    "        normalization_function = config.get(\"normalization_function\")\n",
    "        normalization_function_kwargs = config.get(\"normalization_function_kwargs\", {})\n",
    "\n",
    "        linear = nn.Linear(current_dim, hidden_size)\n",
    "        layers.append(linear)\n",
    "\n",
    "        if normalization_function:\n",
    "            normalization = getattr(nn, normalization_function)(hidden_size, **normalization_function_kwargs)\n",
    "            layers.append(normalization)\n",
    "\n",
    "        if activation_function:\n",
    "            activation = getattr(nn, activation_function)(**activation_function_kwargs)\n",
    "            layers.append(activation)\n",
    "\n",
    "        if dropout_p > 0.0:\n",
    "            dropout = nn.Dropout(p=dropout_p)\n",
    "            layers.append(dropout)\n",
    "\n",
    "        current_dim = hidden_size\n",
    "\n",
    "    return nn.Sequential(*layers), current_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fb506-cb39-4c8c-8ade-d6c1d16c7d47",
   "metadata": {},
   "source": [
    "The next code block involves defining a config and model class following huggingface transformer's class structure [[3]](https://huggingface.co/docs/transformers/custom_models). This allows us to leverage its Trainer class for training and evaluating our models instead of writing custom training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a29232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTabularModelConfig(PretrainedConfig):\n",
    "\n",
    "    model_type = \"text_tabular\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_config=None,\n",
    "        mlp_config=None,\n",
    "        num_labels=2,\n",
    "        model_name=\"distilbert-base-uncased\",\n",
    "        pretrained_model_config=None,\n",
    "        use_pretrained=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.features_config = features_config\n",
    "        self.mlp_config = mlp_config\n",
    "        self.num_labels = num_labels\n",
    "        self.model_name = model_name\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.pretrained_model_config = pretrained_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3d511c-f038-4d4c-afb0-b75f8b5f9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    \"\"\"\n",
    "    Perform mean pooling over sequence len dimension, exclude ones that are attention masked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    last_hidden_state : tensor\n",
    "        Size of [batch_size, sequence_len, hidden dimension]\n",
    "\n",
    "    attention_mask : tensor\n",
    "        Size of [batch size, sequence_len]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embedding : tensor\n",
    "        Size of [batch size, hidden dimension]\n",
    "    \"\"\"\n",
    "    # [..., None] is the same as unsqueeze last dimension\n",
    "    last_hidden_masked = last_hidden_state.masked_fill(~attention_mask.unsqueeze(dim=-1).bool(), 0.0)\n",
    "    embedding = last_hidden_masked.sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(dim=-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d1616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTabularModel(PreTrainedModel):\n",
    "\n",
    "    config_class = TextTabularModelConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        if config.use_pretrained:\n",
    "            self.bert = AutoModel.from_pretrained(config.model_name)\n",
    "        else:\n",
    "            bert_config = AutoConfig.from_pretrained(config.model_name, **config.pretrained_model_config)\n",
    "            self.bert = AutoModel.from_config(bert_config)\n",
    "        \n",
    "        self.categorical_embeddings, categorical_dim, numerical_dim = self.init_tabular_parameters(\n",
    "            config.features_config\n",
    "        )\n",
    "\n",
    "        # note, different pre-trained models might have different attribute name for hidden dimension\n",
    "        text_hidden_dim = self.bert.config.dim\n",
    "        mlp_hidden_dim = text_hidden_dim + categorical_dim + numerical_dim\n",
    "        self.mlp, output_dim = get_mlp_layers(mlp_hidden_dim, config.mlp_config)\n",
    "        self.head = nn.Linear(output_dim, config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tabular_inputs, labels=None):\n",
    "        last_hidden_state = self.bert(input_ids, attention_mask).last_hidden_state\n",
    "        text_embedding = mean_pool(last_hidden_state, attention_mask)\n",
    "        \n",
    "        categorical_inputs, numerical_inputs = self.create_tabular_inputs(\n",
    "            tabular_inputs,\n",
    "            self.config.features_config\n",
    "        )\n",
    "        \n",
    "        if len(categorical_inputs) > 0 and len(numerical_inputs) > 0:\n",
    "            text_tabular_inputs = torch.cat([text_embedding, categorical_inputs, numerical_inputs], dim=-1)\n",
    "        elif len(categorical_inputs) > 0:\n",
    "            text_tabular_inputs = torch.cat([text_embedding, categorical_inputs], dim=-1)\n",
    "        elif len(numerical_inputs) > 0:\n",
    "            text_tabular_inputs = torch.cat([text_embedding, numerical_inputs], dim=-1)\n",
    "\n",
    "        mlp_outputs = self.mlp(text_tabular_inputs)\n",
    "        logits = self.head(mlp_outputs)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        # at the bare minimum, we need to return loss as well as logits\n",
    "        # for both training and evaluation\n",
    "        return loss, F.softmax(logits, dim=-1)\n",
    "\n",
    "    def create_tabular_inputs(self, tabular_inputs, features_config):\n",
    "        numerical_inputs = []\n",
    "        categorical_inputs = []\n",
    "        for name, config in features_config.items():\n",
    "            if config[\"dtype\"] == \"categorical\":\n",
    "                feature_name = f\"{name}_embedding\"\n",
    "                share_embedding = config.get(\"share_embedding\")\n",
    "                if share_embedding:\n",
    "                    feature_name = f\"{share_embedding}_embedding\"\n",
    "\n",
    "                embedding = self.categorical_embeddings[feature_name]\n",
    "                features = tabular_inputs[name].type(torch.long)\n",
    "                embed = embedding(features)                \n",
    "                categorical_inputs.append(embed)\n",
    "            elif config[\"dtype\"] == \"numerical\":\n",
    "                features = tabular_inputs[name].type(torch.float32)\n",
    "                if len(features.shape) == 1:\n",
    "                    features = features.unsqueeze(dim=1)\n",
    "\n",
    "                numerical_inputs.append(features)\n",
    "\n",
    "        if numerical_inputs:\n",
    "            numerical_inputs = torch.cat(numerical_inputs, dim=-1)\n",
    "\n",
    "        if categorical_inputs:\n",
    "            categorical_inputs = torch.cat(categorical_inputs, dim=-1)\n",
    "\n",
    "        return categorical_inputs, numerical_inputs\n",
    "\n",
    "    def init_tabular_parameters(self, features_config):\n",
    "        embeddings = {}\n",
    "        categorical_dim = 0\n",
    "        numerical_dim = 0\n",
    "        for name, config in features_config.items():\n",
    "            if config[\"dtype\"] == \"categorical\":\n",
    "                # create new embedding layer for categorical features if share_embedding is None\n",
    "                share_embedding = config.get(\"share_embedding\")\n",
    "                if share_embedding:\n",
    "                    share_embedding_config = tabular_features_config[share_embedding]\n",
    "                    embedding_size = share_embedding_config[\"embedding_size\"]\n",
    "                else:\n",
    "                    embedding_size = config[\"embedding_size\"]\n",
    "                    embedding = nn.Embedding(config[\"vocab_size\"], embedding_size)\n",
    "                    embedding_name = f\"{name}_embedding\"\n",
    "                    embeddings[embedding_name] = embedding\n",
    "\n",
    "                categorical_dim += embedding_size\n",
    "            elif config[\"dtype\"] == \"numerical\":\n",
    "                numerical_dim += 1\n",
    "\n",
    "        return nn.ModuleDict(embeddings), categorical_dim, numerical_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4dcc615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  143602190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextTabularModel(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(148598, 768, padding_idx=148597)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (categorical_embeddings): ModuleDict(\n",
       "    (gender_embedding): Embedding(4, 4)\n",
       "    (age_embedding): Embedding(7, 4)\n",
       "    (advertiser_id_embedding): Embedding(12193, 32)\n",
       "    (user_id_embedding): Embedding(202547, 64)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=873, out_features=1024, bias=True)\n",
       "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (9): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (head): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_config = [\n",
    "    {\n",
    "        \"hidden_size\": 1024,\n",
    "        \"dropout_p\": 0.1,\n",
    "        \"activation_function\": \"ReLU\",\n",
    "        \"normalization_function\": \"LayerNorm\"\n",
    "    },\n",
    "    {\n",
    "        \"hidden_size\": 512,\n",
    "        \"dropout_p\": 0.1,\n",
    "        \"activation_function\": \"ReLU\",\n",
    "        \"normalization_function\": \"LayerNorm\"\n",
    "    },\n",
    "    {\n",
    "        \"hidden_size\": 256,\n",
    "        \"dropout_p\": 0.1,\n",
    "        \"activation_function\": \"ReLU\",\n",
    "        \"normalization_function\": \"LayerNorm\"\n",
    "    }\n",
    "]\n",
    "pretrained_model_config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"n_layers\": n_layers,\n",
    "    \"pad_token_id\": pad_token_id\n",
    "}\n",
    "text_tabular_config = TextTabularModelConfig(\n",
    "    features_config=features_config,\n",
    "    mlp_config=mlp_config,\n",
    "    pretrained_model_config=pretrained_model_config,\n",
    "    use_pretrained=False\n",
    ")\n",
    "text_tabular_model = TextTabularModel(text_tabular_config)\n",
    "print(\"number of parameters: \", text_tabular_model.num_parameters())\n",
    "text_tabular_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b3f3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6199, grad_fn=<NllLossBackward0>),\n",
       " tensor([[0.5591, 0.4409],\n",
       "         [0.5177, 0.4823]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick test on a sample batch to ensure model forward pass runs\n",
    "output = text_tabular_model(**batch)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff41488-9cb4-41fa-9f0c-a5b16495bbc9",
   "metadata": {},
   "source": [
    "Rest of the code block defines boilerplate code for leveraging huggingface transformer's Trainer, as well as defining a `compute_metrics` function for calculating standard binary classification related metrics.\n",
    "\n",
    "We'll also train a `AutoModelForSequenceClassification` model as our baseline. This allows us to compare whether incorporating tabular features on top of text features will result in performance gains.\n",
    "\n",
    "There're many ways tips and tricks for training our model in a transfer learning setting [[7]](https://arxiv.org/abs/1905.05583) [[8]](https://www.masterfulai.com/blog/getting-the-most-out-of-transfer-learning) [[9]](https://medium.com/mantisnlp/going-the-extra-mile-lessons-learnt-from-kaggle-on-how-to-train-better-nlp-models-part-ii-abf38153c0ed). e.g.\n",
    "\n",
    "- Two stage training. The model is first trained to convergence with a frozen encoder, then unfrozen and fine-tuned along with the entire model.\n",
    "- Differential (a.k.a layer-wise) learning rate. The idea is to have different learning rates for different parts/layers of our model. Intuition behind this is initial layers in a pre-trained model likely have learned general features which we don't want to modify as much, hence we should be setting a smaller learning rate for those parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b6b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, round_digits: int = 3):\n",
    "    y_pred, y_true = eval_preds\n",
    "    y_score = y_pred[:, 1]\n",
    "\n",
    "    log_loss = round(metrics.log_loss(y_true, y_score), round_digits)\n",
    "    roc_auc = round(metrics.roc_auc_score(y_true, y_score), round_digits)\n",
    "    pr_auc = round(metrics.average_precision_score(y_true, y_score), round_digits)\n",
    "    return {\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"log_loss\": log_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d7ade94-8c7a-427d-959a-9dd4d7c4bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differential learning rate, lower learning rate for bert part\n",
    "# of the network, given our bert is not a pre-trained model, we\n",
    "# won't use layer-wise decreasing learning rate\n",
    "bert_params = []\n",
    "other_params = []\n",
    "for name, param in text_tabular_model.named_parameters():\n",
    "    if \"bert.\" in name:\n",
    "        bert_params.append(param)\n",
    "    else:\n",
    "        other_params.append(param)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': bert_params, \"lr\": 0.0001},\n",
    "    {'params': other_params, \"lr\": 0.001}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd200158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14062' max='14062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14062/14062 37:18, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.199228</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>91.491700</td>\n",
       "      <td>1092.995000</td>\n",
       "      <td>136.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.202408</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>91.507000</td>\n",
       "      <td>1092.812000</td>\n",
       "      <td>136.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.199665</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>92.473900</td>\n",
       "      <td>1081.386000</td>\n",
       "      <td>135.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.195827</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>90.559000</td>\n",
       "      <td>1104.253000</td>\n",
       "      <td>138.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.194292</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>92.309600</td>\n",
       "      <td>1083.311000</td>\n",
       "      <td>135.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.192818</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>89.654800</td>\n",
       "      <td>1115.389000</td>\n",
       "      <td>139.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.192244</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>91.605200</td>\n",
       "      <td>1091.641000</td>\n",
       "      <td>136.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.193351</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>90.521800</td>\n",
       "      <td>1104.706000</td>\n",
       "      <td>138.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.193508</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>90.866100</td>\n",
       "      <td>1100.520000</td>\n",
       "      <td>137.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.192026</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>90.745200</td>\n",
       "      <td>1101.987000</td>\n",
       "      <td>137.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.191573</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>90.199200</td>\n",
       "      <td>1108.657000</td>\n",
       "      <td>138.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.194737</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>90.359400</td>\n",
       "      <td>1106.692000</td>\n",
       "      <td>138.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.190699</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>90.229900</td>\n",
       "      <td>1108.280000</td>\n",
       "      <td>138.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.191811</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>87.963800</td>\n",
       "      <td>1136.832000</td>\n",
       "      <td>142.104000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"TRUE\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text_tabular\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    do_train=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # we are collecting all tabular features into a single entry\n",
    "    # tabular_inputs during collate function, this is to prevent\n",
    "    # huggingface trainer from removing these features while processing\n",
    "    # our dataset\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    text_tabular_model,\n",
    "    args=training_args,\n",
    "    data_collator=text_tabular_collate_fn,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, None)\n",
    ")\n",
    "\n",
    "# 0.709\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9efa3f4e-a261-4bb2-ad59-cf529f9812a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Use in conjunction with Dataloader's collate_fn for text data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch : dict\n",
    "        Dictionary with 3 primary keys: input_ids, attention_mask, and labels. e.g.\n",
    "        {\n",
    "            'input_ids': tensor([[116438,  65110]]),\n",
    "            'attention_mask': tensor([[1,  1]]),\n",
    "            'labels': tensor([0, 0])\n",
    "        }\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    labels = []\n",
    "    texts = []\n",
    "    token_ids = []\n",
    "    for example in batch:\n",
    "        label = example[\"label\"]\n",
    "        labels.append(label)\n",
    "\n",
    "        text = \"\"\n",
    "        token_id = []\n",
    "        for name, config in features_config.items():\n",
    "            dtype = config[\"dtype\"]\n",
    "            feature = example[name]\n",
    "            if dtype == \"text\":\n",
    "                text += feature\n",
    "            elif dtype == \"token_id\":\n",
    "                token_id += feature\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "        if token_id:\n",
    "            token_id = torch.LongTensor(token_id)\n",
    "            token_ids.append(token_id)\n",
    "\n",
    "    if texts:\n",
    "        tokenized = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "        output[\"input_ids\"] = tokenized[\"input_ids\"]\n",
    "        output[\"attention_mask\"] = tokenized[\"attention_mask\"]\n",
    "\n",
    "    if token_ids:\n",
    "        input_ids = pad_sequence(token_ids, batch_first=True, padding_value=pad_token_id)\n",
    "        attention_mask = (input_ids != pad_token_id).type(torch.int)\n",
    "        output[\"input_ids\"] = input_ids\n",
    "        output[\"attention_mask\"] = attention_mask\n",
    "\n",
    "    output[\"labels\"] = torch.LongTensor(labels)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7871241a-8ede-4672-851d-c036cd74d335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  129285890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14062' max='14062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14062/14062 29:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.199994</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>69.422000</td>\n",
       "      <td>1440.465000</td>\n",
       "      <td>180.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.199953</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>71.157800</td>\n",
       "      <td>1405.327000</td>\n",
       "      <td>175.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.197143</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>71.895300</td>\n",
       "      <td>1390.911000</td>\n",
       "      <td>173.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.198350</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>70.564600</td>\n",
       "      <td>1417.142000</td>\n",
       "      <td>177.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.195767</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>70.091500</td>\n",
       "      <td>1426.706000</td>\n",
       "      <td>178.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>70.318200</td>\n",
       "      <td>1422.107000</td>\n",
       "      <td>177.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.194443</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>71.554400</td>\n",
       "      <td>1397.538000</td>\n",
       "      <td>174.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.198204</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>69.429100</td>\n",
       "      <td>1440.317000</td>\n",
       "      <td>180.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.200503</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>69.567300</td>\n",
       "      <td>1437.457000</td>\n",
       "      <td>179.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.196297</td>\n",
       "      <td>0.671000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>72.405600</td>\n",
       "      <td>1381.109000</td>\n",
       "      <td>172.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.199696</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>69.738600</td>\n",
       "      <td>1433.927000</td>\n",
       "      <td>179.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.196726</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>70.564800</td>\n",
       "      <td>1417.136000</td>\n",
       "      <td>177.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>0.195188</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>70.021300</td>\n",
       "      <td>1428.136000</td>\n",
       "      <td>178.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.196994</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>71.060200</td>\n",
       "      <td>1407.257000</td>\n",
       "      <td>175.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_config = AutoConfig.from_pretrained(model_name, **pretrained_model_config)\n",
    "text_model = AutoModelForSequenceClassification.from_config(bert_config)\n",
    "print(\"number of parameters: \", text_model.num_parameters())\n",
    "\n",
    "trainer = Trainer(\n",
    "    text_model,\n",
    "    args=training_args,\n",
    "    data_collator=text_collate_fn,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877f3b9-ce19-46b9-8adf-c1754f10a182",
   "metadata": {},
   "source": [
    "From the experiment above, including some tabular/structure data into the model out-performs the text only variant by 300 basis point (1 absolute percent) 0.709 versus 0.679 on this particular preprocessed dataset. The gap will most likely widen even more if we devote more effort into feature engineering additional features out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f770c-3ead-4ec7-9dd8-c301b74f6b87",
   "metadata": {},
   "source": [
    "## End Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d80df-0ce3-48a5-ae08-8e96a2e03d00",
   "metadata": {},
   "source": [
    "Recent works such as DeText [[4]](https://arxiv.org/abs/2008.02460), CTR-BERT [[5]](https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf), TwinBERT [[6]](https://arxiv.org/abs/2002.06275) also explored leveraging BERT for click through rate (CTR) problems. Some common themes from these works includes:\n",
    "\n",
    "- Cross architecture knowledge distillation process, where large cross attention teacher are distilled to smaller bi-encoder student. Both teacher and student incorporates tabular features via late fusion manner. One of the considerations in building industry scale CTR prediction model is achieving fast real time inference. Bi-encoder model address this by decoupling the encoding process for queries and documents. This enables one of the computationally expensive BERT-based document encoders to be pre-computed offline and cached (cache can be refreshed on a periodic basis, e.g. daily). During run time, the inference cost reduces to a query encoder, tabular features, as well as a late fusion cross layer such as MLP (multi-layer perceptron).\n",
    "- N-pass ranker. To leverage cross encoder in production setting, we can have a N-pass ranker. i.e. A more lightweight ranker that can quickly weed out a large amounts of irrelevant document, while applying the more performant but computationally expensive ranker on a smaller subset of the documents.\n",
    "- In domain pre-training. A common practice of using pre-trained BERT model is to directly leverage the public checkpoint that have been trained on general domain such as Wikipedia, and fine-tune it on our specific task. Continuing to pre-train on in-domain data using un/self-supervised learning loss like MLM (Masked Language Modeling) proves to be effective for vertical specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15e766",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe6eea",
   "metadata": {},
   "source": [
    "- [[1]](https://www.kaggle.com/competitions/kddcup2012-track2/overview) KDD Cup 2012, Track 2 - Predict the click-through rate of ads given the query and user information.\n",
    "- [[2]](https://arxiv.org/abs/2207.03051) Haitao Mao, Dawei Yin, Xiaokai Chu, et al. - A Large Scale Search Dataset for Unbiased Learning to Rank (2022)\n",
    "- [[3]](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4) How to Incorporate Tabular Data with HuggingFace Transformers\n",
    "- [[4]](https://arxiv.org/abs/2008.02460) Weiwei Guo, Xiaowei Liu, Sida Wang, Huiji Gao, Ananth Sankar, Zimeng Yang, Qi Guo, Liang Zhang, Bo Long, Bee-Chung Chen, Deepak Agarwal - DeText: A Deep Text Ranking Framework with BERT (2020)\n",
    "- [[5]](https://neurips2021-nlp.github.io/papers/20/CameraReady/camera_ready_final.pdf) Aashiq Muhamed, et al. - CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models (2021)\n",
    "- [[6]](https://arxiv.org/abs/2002.06275) Wenhao Lu, Jian Jiao, Ruofei Zhang - TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval (2022)\n",
    "- [[7]](https://arxiv.org/abs/1905.05583) Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang - How to Fine-Tune BERT for Text Classification? (2019)\n",
    "- [[8]](https://www.masterfulai.com/blog/getting-the-most-out-of-transfer-learning) Getting the most out of Transfer Learning\n",
    "- [[9]](https://medium.com/mantisnlp/going-the-extra-mile-lessons-learnt-from-kaggle-on-how-to-train-better-nlp-models-part-ii-abf38153c0ed) Going the extra mile, lessons learnt from Kaggle on how to train better NLP models (Part II)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
